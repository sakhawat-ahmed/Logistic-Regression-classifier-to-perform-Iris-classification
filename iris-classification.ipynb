{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3ecfb4-c1e4-4f0a-8c3f-149248631b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25done\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/sakhawat/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /home/sakhawat/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sakhawat/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "done\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813860 sha256=8b7ff4554ddbc35bfe2af78c1e2b4bf9df7a11d8d7ac8ff2c9008c48cd79d77b\n",
      "  Stored in directory: /home/sakhawat/.cache/pip/wheels/31/9f/68/f89fb34ccd886909be7d0e390eaaf97f21efdf540c0ee8dbcd\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5653f2b-13e0-430e-910f-8c4d525a3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "features = pd.DataFrame(iris.data, columns=iris.feature_names)   # requirement (1)\n",
    "target   = pd.DataFrame(iris.target, columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e314b59-cd0a-4ce8-9367-60baada069e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.concat([features, target], axis=1)   # column-wise concat\n",
    "iris_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379ac469-2461-4096-b989-87afb8d2703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/05 23:01:26 WARN Utils: Your hostname, sakhawat, resolves to a loopback address: 127.0.1.1; using 10.147.114.173 instead (on interface wlp3s0)\n",
      "25/11/05 23:01:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 23:01:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal length (cm): double (nullable = true)\n",
      " |-- sepal width (cm): double (nullable = true)\n",
      " |-- petal length (cm): double (nullable = true)\n",
      " |-- petal width (cm): double (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|label|\n",
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "|              5.1|             3.5|              1.4|             0.2|    0|\n",
      "|              4.9|             3.0|              1.4|             0.2|    0|\n",
      "|              4.7|             3.2|              1.3|             0.2|    0|\n",
      "|              4.6|             3.1|              1.5|             0.2|    0|\n",
      "|              5.0|             3.6|              1.4|             0.2|    0|\n",
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iris_Classification_PySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_df = spark.createDataFrame(iris_df)\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c40e873-1b4c-4586-ab75-a9876f1fdd47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_df\u001b[38;5;241m.\u001b[39mcount(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_df\u001b[38;5;241m.\u001b[39mcount())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Train count:\", train_df.count(), \"Test count:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2003c-7dad-46ce-be77-a11d63b12259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "model = lr.fit(train_df)   # fit distributed model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75751946-b86a-4123-8c54-f2ff555d60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (model, test_df)\n",
    "# result[0] is the trained model, result[1] is the Spark DataFrame test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f4acd-df9c-469a-a6fe-0e23e4bd2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)\n",
    "# show sample\n",
    "predictions.select(\"features\",\"label\",\"prediction\",\"probability\").show(5)\n",
    "\n",
    "# compute accuracy with MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2303c253-e811-4c2f-aefe-8bed7732cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined pandas DataFrame created successfully.\n",
      "‚úÖ Spark session started.\n",
      "‚úÖ Converted to Spark DataFrame.\n",
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|label|\n",
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "|              5.1|             3.5|              1.4|             0.2|    0|\n",
      "|              4.9|             3.0|              1.4|             0.2|    0|\n",
      "|              4.7|             3.2|              1.3|             0.2|    0|\n",
      "|              4.6|             3.1|              1.5|             0.2|    0|\n",
      "|              5.0|             3.6|              1.4|             0.2|    0|\n",
      "+-----------------+----------------+-----------------+----------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Could not parse formula: label ~ sepal length (cm) + sepal width (cm) + petal length (cm) + petal width (cm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mfeature_names\n\u001b[1;32m     38\u001b[0m formula \u001b[38;5;241m=\u001b[39m RFormula(formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel ~ \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m + \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(feature_cols))\n\u001b[0;32m---> 39\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m formula\u001b[38;5;241m.\u001b[39mfit(spark_df)\n\u001b[1;32m     40\u001b[0m final_df \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mtransform(spark_df)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ RFormula applied successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/ml/util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, dataset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/ml/wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/ml/wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Could not parse formula: label ~ sepal length (cm) + sepal width (cm) + petal length (cm) + petal width (cm)"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Project 1 ‚Äî Iris Classification using PySpark ML LogisticRegression\n",
    "# ============================================================\n",
    "\n",
    "# Step 1. Import all required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Step 2. Load the Iris dataset from scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# Step 3. Convert numpy arrays to pandas DataFrames\n",
    "features = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "target = pd.DataFrame(iris.target, columns=[\"label\"])\n",
    "\n",
    "# Step 4. Combine features and target using concat\n",
    "iris_df = pd.concat([features, target], axis=1)\n",
    "print(\"‚úÖ Combined pandas DataFrame created successfully.\")\n",
    "\n",
    "# Step 5. Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iris_Classification_PySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "print(\"‚úÖ Spark session started.\")\n",
    "\n",
    "# Step 6. Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(iris_df)\n",
    "print(\"‚úÖ Converted to Spark DataFrame.\")\n",
    "spark_df.show(5)\n",
    "\n",
    "# Step 7. Use RFormula to map feature columns -> features and target -> label\n",
    "feature_cols = iris.feature_names\n",
    "formula = RFormula(formula=\"label ~ \" + \" + \".join(feature_cols))\n",
    "rf_model = formula.fit(spark_df)\n",
    "final_df = rf_model.transform(spark_df)\n",
    "print(\"‚úÖ RFormula applied successfully.\")\n",
    "final_df.select(\"features\", \"label\").show(5)\n",
    "\n",
    "# Step 8. Split the data into training (80%) and test (20%)\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"‚úÖ Data split: {train_df.count()} training rows, {test_df.count()} test rows\")\n",
    "\n",
    "# Step 9. Initialize and train (fit) LogisticRegression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "model = lr.fit(train_df)\n",
    "print(\"‚úÖ Model training completed.\")\n",
    "\n",
    "# Step 10. Return the tuple (model, test_df)\n",
    "result = (model, test_df)\n",
    "print(\"‚úÖ Tuple (model, test_df) created successfully.\")\n",
    "\n",
    "# Step 11. Test model predictions\n",
    "predictions = model.transform(test_df)\n",
    "print(\"‚úÖ Predictions generated.\")\n",
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Step 12. Evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"üéØ Model accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "# Optional: show more evaluation metrics\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "print(f\"üîπ F1 Score: {evaluator_f1.evaluate(predictions):.4f}\")\n",
    "\n",
    "# Done\n",
    "print(\"\\n‚úÖ Project completed successfully! Logistic Regression model is trained and evaluated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f93340-948a-43c1-8b8d-1523f32d31bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
